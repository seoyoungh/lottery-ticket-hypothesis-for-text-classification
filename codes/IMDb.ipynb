{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IMDb-new-version.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMSOcFNp7WbF4L+UX5I70re"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"CJ92D2O6Jqkn","colab_type":"code","outputId":"379880ab-8fe8-4210-b1d5-35f841195271","executionInfo":{"status":"ok","timestamp":1587955583811,"user_tz":-540,"elapsed":1099,"user":{"displayName":"Seoyoung Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjITIRuMxnapYLrcCepnJsCF3Rb52KogXDd9B-Mhg=s64","userId":"04592586031163595245"}},"colab":{"base_uri":"https://localhost:8080/","height":32}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UrbKXIUGWS3R","colab_type":"code","colab":{}},"source":["import torch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mVTivRv2xPpU","colab_type":"code","outputId":"ebd15dc7-1f3b-4349-e89f-7320d05542e2","executionInfo":{"status":"ok","timestamp":1587955591195,"user_tz":-540,"elapsed":1120,"user":{"displayName":"Seoyoung Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjITIRuMxnapYLrcCepnJsCF3Rb52KogXDd9B-Mhg=s64","userId":"04592586031163595245"}},"colab":{"base_uri":"https://localhost:8080/","height":65}},"source":["# GPU Check\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print ('Available devices ', torch.cuda.device_count())\n","print ('Current cuda device ', torch.cuda.current_device())\n","print(torch.cuda.get_device_name(device))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Available devices  1\n","Current cuda device  0\n","Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OU8OIZFNxM5w","colab_type":"code","outputId":"e0d38cb2-f09f-40ae-83c7-47aa78a05e23","executionInfo":{"status":"ok","timestamp":1587949540078,"user_tz":-540,"elapsed":652,"user":{"displayName":"Seoyoung Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjITIRuMxnapYLrcCepnJsCF3Rb52KogXDd9B-Mhg=s64","userId":"04592586031163595245"}},"colab":{"base_uri":"https://localhost:8080/","height":32}},"source":["# TPU Check\n","\n","assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n","TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","print('TPU address is', TPU_ADDRESS)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TPU address is grpc://10.101.190.90:8470\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ahzAasAIKtAn","colab_type":"code","colab":{}},"source":["import os\n","import time\n","import torch\n","from torchtext import data\n","from torchtext import datasets\n","from torchtext.vocab import GloVe\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c86bvB2xLpYQ","colab_type":"code","outputId":"5962078d-d81b-4b97-843c-77ae5f0dd495","executionInfo":{"status":"error","timestamp":1588095940590,"user_tz":-540,"elapsed":636,"user":{"displayName":"Seoyoung Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjITIRuMxnapYLrcCepnJsCF3Rb52KogXDd9B-Mhg=s64","userId":"04592586031163595245"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["import random\n","\n","SEED = 1234 # Random Seed for reproductivity\n","\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","# tockenize 지정해주기\n","TEXT = data.Field(sequential=True, tokenize='spacy', lower=True, include_lengths=True, batch_first=True, fix_length=200)\n","LABEL = data.LabelField(dtype = torch.float)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-8bc47fd9eb3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mSEED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1234\u001b[0m \u001b[0;31m# Random Seed for reproductivity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeterministic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}]},{"cell_type":"code","metadata":{"id":"9mql-moTOP1I","colab_type":"code","outputId":"670c86ba-4410-448e-ff7b-ce9c5ebea230","executionInfo":{"status":"ok","timestamp":1587955883118,"user_tz":-540,"elapsed":88989,"user":{"displayName":"Seoyoung Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjITIRuMxnapYLrcCepnJsCF3Rb52KogXDd9B-Mhg=s64","userId":"04592586031163595245"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["train, test = datasets.IMDB.splits(TEXT, LABEL)\n","\n","print(f'Number of training examples: {len(train)}')\n","print(f'Number of testing examples: {len(test)}')\n","\n","print(vars(train.examples[0]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of training examples: 25000\n","Number of testing examples: 25000\n","{'text': ['please', 'see', 'also', 'my', 'comment', 'on', 'die', 'nibelungen', 'part', '1', ':', 'siegfried.<br', '/><br', '/>the', 'second', 'part', 'of', 'ufa', 'studio', \"'s\", 'gargantuan', 'production', 'of', 'the', 'nibelungen', 'saga', 'continues', 'in', 'the', 'stylised', ',', 'symphonic', 'and', 'emotionally', 'detached', 'manner', 'of', 'its', 'predecessor', '.', 'however', ',', 'whereas', 'part', 'one', 'was', 'a', 'passionless', 'portrayal', 'of', 'individual', 'acts', 'of', 'heroism', ',', 'part', 'two', 'is', 'a', 'chaotic', 'depiction', 'of', 'bloodletting', 'on', 'a', 'grand', 'scale.<br', '/><br', '/>as', 'in', 'part', 'one', ',', 'director', 'fritz', 'lang', 'maintains', 'a', 'continuous', 'dynamic', 'rhythm', ',', 'with', 'the', 'pace', 'of', 'the', 'action', 'and', 'the', 'complexity', 'of', 'the', 'shot', 'composition', 'rising', 'and', 'falling', 'smoothly', 'as', 'the', 'tone', 'of', 'each', 'scene', 'demands', '.', 'these', 'pictures', 'should', 'only', 'be', 'watched', 'with', 'the', 'note', '-', 'perfect', 'gottfried', 'huppertz', 'score', ',', 'which', 'fortunately', 'is', 'on', 'the', 'kino', 'dvd', '.', 'now', ',', 'with', 'this', 'focus', 'on', 'mass', 'action', ',', 'lang', 'is', 'presented', 'with', 'greater', 'challenges', 'in', 'staging', '.', 'the', 'action', 'sequences', 'in', 'his', 'earliest', 'features', 'were', 'often', 'badly', 'constructed', ',', 'but', 'now', 'he', 'simply', 'makes', 'them', 'part', 'of', 'that', 'rhythmic', 'flow', ',', 'with', 'the', 'level', 'of', 'activity', 'on', 'the', 'screen', 'swelling', 'up', 'like', 'an', 'orchestra.<br', '/><br', '/>but', 'just', 'as', 'part', 'one', 'made', 'us', 'witness', 'siegfried', \"'s\", 'adventures', 'matter', '-', 'of', '-', 'factly', 'and', 'without', 'excitement', ',', 'part', 'two', 'presents', 'warfare', 'as', 'devastating', 'tragedy', '.', 'in', 'both', 'pictures', ',', 'there', 'is', 'a', 'deliberate', 'lack', 'of', 'emotional', 'connection', 'with', 'the', 'characters', '.', 'that', \"'s\", 'why', 'lang', 'mostly', 'keeps', 'the', 'camera', 'outside', 'of', 'the', 'action', ',', 'never', 'allowing', 'us', 'to', 'feel', 'as', 'if', 'we', 'are', 'there', '(', 'and', 'this', 'is', 'significant', 'because', 'involving', 'the', 'audience', 'is', 'normally', 'a', 'distinction', 'of', 'lang', \"'s\", 'work', ')', '.', 'that', \"'s\", 'also', 'why', 'the', 'performances', 'are', 'unnaturally', 'theatrical', ',', 'with', 'the', 'actors', 'lurching', 'around', 'like', 'constipated', 'sleepwalkers.<br', '/><br', '/>nevertheless', ',', 'kriemhild', \"'s\", 'revenge', 'does', 'constantly', 'deal', 'with', 'emotions', ',', 'and', 'is', 'in', 'fact', 'profoundly', 'humanist', '.', 'the', 'one', 'moment', 'of', 'naturalism', 'is', 'when', 'atilla', 'holds', 'his', 'baby', 'son', 'for', 'the', 'first', 'time', ',', 'and', 'lang', 'actually', 'emphasises', 'the', 'tenderness', 'of', 'this', 'scene', 'by', 'building', 'up', 'to', 'it', 'with', 'the', 'wild', ',', 'frantic', 'ride', 'of', 'the', 'huns', '.', 'the', 'point', 'is', 'that', 'lang', 'never', 'manipulates', 'us', 'into', 'taking', 'sides', ',', 'and', 'in', 'that', 'respect', 'this', 'version', 'has', 'more', 'in', 'common', 'with', 'the', 'original', 'saga', 'than', 'the', 'wagner', 'opera', '.', 'the', 'climactic', 'slaughter', 'is', 'the', 'very', 'antithesis', 'of', 'a', 'rousing', 'battle', 'scene', '.', 'why', 'then', 'did', 'hitler', 'and', 'co.', 'get', 'so', 'teary', '-', 'eyed', 'over', 'it', ',', 'a', 'fact', 'which', 'has', 'unfairly', 'tarnished', 'the', 'reputation', 'of', 'these', 'films', '?', 'because', 'the', 'unwavering', 'racial', 'ideology', 'of', 'the', 'nazis', 'made', 'them', 'automatically', 'view', 'the', 'nibelungs', 'as', 'the', 'good', 'guys', ',', 'even', 'if', 'they', 'do', 'kill', 'babies', 'and', 'betray', 'their', 'own', 'kin', '.', 'for', 'hitler', ',', 'their', 'downfall', 'would', 'always', 'be', 'a', 'nationalist', 'tragedy', ',', 'not', 'a', 'human', 'one.<br', '/><br', '/>but', 'for', 'us', 'non', '-', 'nazi', 'viewers', ',', 'what', 'makes', 'this', 'picture', 'enjoyable', 'is', 'its', 'beautiful', 'sense', 'of', 'pageantry', 'and', 'musical', 'rhythm', '.', 'when', 'you', 'see', 'these', 'fully', '-', 'developed', 'silent', 'pictures', 'of', 'lang', \"'s\", ',', 'it', 'makes', 'you', 'realise', 'how', 'much', 'he', 'was', 'wasted', 'in', 'hollywood', '.', 'rather', 'than', 'saddling', 'him', 'with', 'low', '-', 'budget', 'potboilers', ',', 'they', 'should', 'have', 'put', 'him', 'to', 'work', 'on', 'a', 'few', 'of', 'those', 'sword', '-', 'and', '-', 'sandal', 'epics', ',', 'pictures', 'that', 'do', 'not', 'have', 'to', 'be', 'believable', 'and', 'do', 'not', 'have', 'to', 'move', 'us', 'emotionally', ',', 'where', 'it', \"'s\", 'the', 'poetic', ',', 'operatic', 'tonality', 'that', 'sweeps', 'us', 'along', '.'], 'label': 'pos'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Y7XQyHVkOSxh","colab_type":"code","colab":{}},"source":["# make splits for data\n","# by default this splits 70:30\n","\n","train, valid = train.split(random_state = random.seed(SEED))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uq0OqwkqOU8r","colab_type":"code","outputId":"844699e0-e5b0-474c-e565-903aabffb877","executionInfo":{"status":"ok","timestamp":1587955897167,"user_tz":-540,"elapsed":794,"user":{"displayName":"Seoyoung Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjITIRuMxnapYLrcCepnJsCF3Rb52KogXDd9B-Mhg=s64","userId":"04592586031163595245"}},"colab":{"base_uri":"https://localhost:8080/","height":82}},"source":["print('train.fields', train.fields)\n","print(f'Number of training examples: {len(train)}')\n","print(f'Number of validation examples: {len(valid)}') \n","print(f'Number of testing examples: {len(test)}')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["train.fields {'text': <torchtext.data.field.Field object at 0x7fcbfdd2f1d0>, 'label': <torchtext.data.field.LabelField object at 0x7fcbfc1bb400>}\n","Number of training examples: 17500\n","Number of validation examples: 7500\n","Number of testing examples: 25000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ey2_yG8FMa2E","colab_type":"code","outputId":"3bae1634-42eb-4c8e-9ea4-9642196da6e0","executionInfo":{"status":"ok","timestamp":1587955902398,"user_tz":-540,"elapsed":3736,"user":{"displayName":"Seoyoung Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjITIRuMxnapYLrcCepnJsCF3Rb52KogXDd9B-Mhg=s64","userId":"04592586031163595245"}},"colab":{"base_uri":"https://localhost:8080/","height":65}},"source":["TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300))\n","LABEL.build_vocab(train)\n","\n","word_embeddings = TEXT.vocab.vectors\n","\n","print (\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n","print (\"Vector size of Text Vocabulary: \", TEXT.vocab.vectors.size())\n","print (\"Label Length: \" + str(len(LABEL.vocab)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Length of Text Vocabulary: 85251\n","Vector size of Text Vocabulary:  torch.Size([85251, 300])\n","Label Length: 2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XBkzAsnnMou8","colab_type":"code","colab":{}},"source":["BATCH_SIZE = 32\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","train_iter, valid_iter, test_iter = data.BucketIterator.splits(\n","    (train, valid, test), \n","    batch_size = BATCH_SIZE,\n","    device = device)\n","\n","vocab_size = len(TEXT.vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M37tX1BJK3cq","colab_type":"code","colab":{}},"source":["# TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_data.load_dataset()\n","\n","def clip_gradient(model, clip_value):\n","    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n","    for p in params:\n","        p.grad.data.clamp_(-clip_value, clip_value)\n","    \n","def train_model(model, train_iter, epoch):\n","    total_epoch_loss = 0\n","    total_epoch_acc = 0\n","    model.cuda()\n","    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n","    steps = 0\n","    model.train()\n","    for idx, batch in enumerate(train_iter):\n","        text = batch.text[0]\n","        target = batch.label\n","        target = torch.autograd.Variable(target).long()\n","        if torch.cuda.is_available():\n","            text = text.cuda()\n","            target = target.cuda()\n","        if (text.size()[0] is not 32):# One of the batch returned by BucketIterator has length different than 32.\n","            continue\n","        optim.zero_grad()\n","        prediction = model(text)\n","        loss = loss_fn(prediction, target)\n","        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n","        acc = 100.0 * num_corrects/len(batch)\n","        loss.backward()\n","        clip_gradient(model, 1e-1)\n","        optim.step()\n","        steps += 1\n","        \n","        if steps % 100 == 0:\n","            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n","        \n","        total_epoch_loss += loss.item()\n","        total_epoch_acc += acc.item()\n","        \n","    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n","\n","def eval_model(model, val_iter):\n","    total_epoch_loss = 0\n","    total_epoch_acc = 0\n","    model.eval()\n","    with torch.no_grad():\n","        for idx, batch in enumerate(val_iter):\n","            text = batch.text[0]\n","            if (text.size()[0] is not 32):\n","                continue\n","            target = batch.label\n","            target = torch.autograd.Variable(target).long()\n","            if torch.cuda.is_available():\n","                text = text.cuda()\n","                target = target.cuda()\n","            prediction = model(text)\n","            loss = loss_fn(prediction, target)\n","            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n","            acc = 100.0 * num_corrects/len(batch)\n","            total_epoch_loss += loss.item()\n","            total_epoch_acc += acc.item()\n","\n","    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1FVqAlbxRm1x","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","from torch.nn import functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Ck5ykmKRKGh","colab_type":"code","colab":{}},"source":["# CNN\n","\n","class CNN(nn.Module):\n","\tdef __init__(self, batch_size, output_size, in_channels, out_channels, kernel_heights, stride, padding, keep_probab, vocab_size, embedding_length):\n","\t\tsuper(CNN, self).__init__()\n","\t\t\n","\t\t\"\"\"\n","\t\tArguments\n","\t\t---------\n","\t\tbatch_size : Size of each batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n","\t\toutput_size : 2 = (pos, neg)\n","\t\tin_channels : Number of input channels. Here it is 1 as the input data has dimension = (batch_size, num_seq, embedding_length)\n","\t\tout_channels : Number of output channels after convolution operation performed on the input matrix\n","\t\tkernel_heights : A list consisting of 3 different kernel_heights. Convolution will be performed 3 times and finally results from each kernel_height will be concatenated.\n","\t\tkeep_probab : Probability of retaining an activation node during dropout operation\n","\t\tvocab_size : Size of the vocabulary containing unique words\n","\t\tembedding_length : Embedding dimension of GloVe word embeddings\n","\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table\n","\t\t--------\n","\t\t\n","\t\t\"\"\"\n","\t\tself.batch_size = batch_size\n","\t\tself.output_size = output_size\n","\t\tself.in_channels = in_channels\n","\t\tself.out_channels = out_channels\n","\t\tself.kernel_heights = kernel_heights\n","\t\tself.stride = stride\n","\t\tself.padding = padding\n","\t\tself.vocab_size = vocab_size\n","\t\tself.embedding_length = embedding_length\n","\t\t\n","\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n","\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False)\n","\t\tself.conv1 = nn.Conv2d(in_channels, out_channels, (kernel_heights[0], embedding_length), stride, padding)\n","\t\tself.conv2 = nn.Conv2d(in_channels, out_channels, (kernel_heights[1], embedding_length), stride, padding)\n","\t\tself.conv3 = nn.Conv2d(in_channels, out_channels, (kernel_heights[2], embedding_length), stride, padding)\n","\t\tself.dropout = nn.Dropout(keep_probab)\n","\t\tself.label = nn.Linear(len(kernel_heights)*out_channels, output_size)\n","\t\n","\tdef conv_block(self, input, conv_layer):\n","\t\tconv_out = conv_layer(input) # conv_out.size() = (batch_size, out_channels, dim, 1)\n","\t\tactivation = F.relu(conv_out.squeeze(3))# activation.size() = (batch_size, out_channels, dim1)\n","\t\tmax_out = F.max_pool1d(activation, activation.size()[2]).squeeze(2)# maxpool_out.size() = (batch_size, out_channels)\n","\t\t\n","\t\treturn max_out\n","\t\n","\tdef forward(self, input_sentences, batch_size=None):\n","\t\t\n","\t\t\"\"\"\n","\t\tThe idea of the Convolutional Neural Netwok for Text Classification is very simple. We perform convolution operation on the embedding matrix \n","\t\twhose shape for each batch is (num_seq, embedding_length) with kernel of varying height but constant width which is same as the embedding_length.\n","\t\tWe will be using ReLU activation after the convolution operation and then for each kernel height, we will use max_pool operation on each tensor \n","\t\tand will filter all the maximum activation for every channel and then we will concatenate the resulting tensors. This output is then fully connected\n","\t\tto the output layers consisting two units which basically gives us the logits for both positive and negative classes.\n","\t\t\n","\t\tParameters\n","\t\t----------\n","\t\tinput_sentences: input_sentences of shape = (batch_size, num_sequences)\n","\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n","\t\t\n","\t\tReturns\n","\t\t-------\n","\t\tOutput of the linear layer containing logits for pos & neg class.\n","\t\tlogits.size() = (batch_size, output_size)\n","\t\t\n","\t\t\"\"\"\n","\t\t\n","\t\tinput = self.word_embeddings(input_sentences)\n","\t\t# input.size() = (batch_size, num_seq, embedding_length)\n","\t\tinput = input.unsqueeze(1)\n","\t\t# input.size() = (batch_size, 1, num_seq, embedding_length)\n","\t\tmax_out1 = self.conv_block(input, self.conv1)\n","\t\tmax_out2 = self.conv_block(input, self.conv2)\n","\t\tmax_out3 = self.conv_block(input, self.conv3)\n","\t\t\n","\t\tall_out = torch.cat((max_out1, max_out2, max_out3), 1)\n","\t\t# all_out.size() = (batch_size, num_kernels*out_channels)\n","\t\tfc_in = self.dropout(all_out)\n","\t\t# fc_in.size()) = (batch_size, num_kernels*out_channels)\n","\t\tlogits = self.label(fc_in)\n","\t\t\n","\t\treturn logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"162cy2rIRw0A","colab_type":"code","colab":{}},"source":["# RNN\n","\n","class RNN(nn.Module):\n","\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n","\t\tsuper(RNN, self).__init__()\n","\n","\t\t\"\"\"\n","\t\tArguments\n","\t\t---------\n","\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n","\t\toutput_size : 2 = (pos, neg)\n","\t\thidden_sie : Size of the hidden_state of the LSTM\n","\t\tvocab_size : Size of the vocabulary containing unique words\n","\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n","\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n","\t\t\n","\t\t\"\"\"\n","\n","\t\tself.batch_size = batch_size\n","\t\tself.output_size = output_size\n","\t\tself.hidden_size = hidden_size\n","\t\tself.vocab_size = vocab_size\n","\t\tself.embedding_length = embedding_length\n","\t\t\n","\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n","\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False)\n","\t\tself.rnn = nn.RNN(embedding_length, hidden_size, num_layers=2, bidirectional=True)\n","\t\tself.label = nn.Linear(4*hidden_size, output_size)\n","\t\n","\tdef forward(self, input_sentences, batch_size=None):\n","\t\t\n","\t\t\"\"\" \n","\t\tParameters\n","\t\t----------\n","\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n","\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n","\t\t\n","\t\tReturns\n","\t\t-------\n","\t\tOutput of the linear layer containing logits for pos & neg class which receives its input as the final_hidden_state of RNN.\n","\t\tlogits.size() = (batch_size, output_size)\n","\t\t\n","\t\t\"\"\"\n","\n","\t\tinput = self.word_embeddings(input_sentences)\n","\t\tinput = input.permute(1, 0, 2)\n","\t\tif batch_size is None:\n","\t\t\th_0 = Variable(torch.zeros(4, self.batch_size, self.hidden_size).cuda()) # 4 = num_layers*num_directions\n","\t\telse:\n","\t\t\th_0 =  Variable(torch.zeros(4, batch_size, self.hidden_size).cuda())\n","\t\toutput, h_n = self.rnn(input, h_0)\n","\t\t# h_n.size() = (4, batch_size, hidden_size)\n","\t\th_n = h_n.permute(1, 0, 2) # h_n.size() = (batch_size, 4, hidden_size)\n","\t\th_n = h_n.contiguous().view(h_n.size()[0], h_n.size()[1]*h_n.size()[2])\n","\t\t# h_n.size() = (batch_size, 4*hidden_size)\n","\t\tlogits = self.label(h_n) # logits.size() = (batch_size, output_size)\n","\t\t\n","\t\treturn logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJB0fTNER4Qy","colab_type":"code","colab":{}},"source":["# LSTM\n","\n","class LSTMClassifier(nn.Module):\n","\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n","\t\tsuper(LSTMClassifier, self).__init__()\n","\t\t\n","\t\t\"\"\"\n","\t\tArguments\n","\t\t---------\n","\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n","\t\toutput_size : 2 = (pos, neg)\n","\t\thidden_sie : Size of the hidden_state of the LSTM\n","\t\tvocab_size : Size of the vocabulary containing unique words\n","\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n","\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n","\t\t\n","\t\t\"\"\"\n","\t\t\n","\t\tself.batch_size = batch_size\n","\t\tself.output_size = output_size\n","\t\tself.hidden_size = hidden_size\n","\t\tself.vocab_size = vocab_size\n","\t\tself.embedding_length = embedding_length\n","\t\t\n","\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)# Initializing the look-up table.\n","\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assigning the look-up table to the pre-trained GloVe word embedding.\n","\t\tself.lstm = nn.LSTM(embedding_length, hidden_size)\n","\t\tself.label = nn.Linear(hidden_size, output_size)\n","\t\t\n","\tdef forward(self, input_sentence, batch_size=None):\n","\t\n","\t\t\"\"\" \n","\t\tParameters\n","\t\t----------\n","\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n","\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n","\t\t\n","\t\tReturns\n","\t\t-------\n","\t\tOutput of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n","\t\tfinal_output.shape = (batch_size, output_size)\n","\t\t\n","\t\t\"\"\"\n","\t\t\n","\t\t''' Here we will map all the indexes present in the input sequence to the corresponding word vector using our pre-trained word_embedddins.'''\n","\t\tinput = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences,  embedding_length)\n","\t\tinput = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n","\t\tif batch_size is None:\n","\t\t\th_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial hidden state of the LSTM\n","\t\t\tc_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial cell state of the LSTM\n","\t\telse:\n","\t\t\th_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n","\t\t\tc_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n","\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n","\t\tfinal_output = self.label(final_hidden_state[-1]) # final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\n","\t\t\n","\t\treturn final_output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"012otSy3MyhY","colab_type":"code","outputId":"7a27d047-0bda-41d2-9cd5-67d065fa8cd0","executionInfo":{"status":"ok","timestamp":1587952147658,"user_tz":-540,"elapsed":204389,"user":{"displayName":"Seoyoung Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjITIRuMxnapYLrcCepnJsCF3Rb52KogXDd9B-Mhg=s64","userId":"04592586031163595245"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["learning_rate = 2e-5\n","batch_size = 32\n","output_size = 2\n","hidden_size = 256\n","embedding_length = 300\n","\n","model = RNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n","loss_fn = F.cross_entropy\n","\n","for epoch in range(10):\n","    train_loss, train_acc = train_model(model, train_iter, epoch)\n","    val_loss, val_acc = eval_model(model, valid_iter)\n","    \n","    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n","    \n","test_loss, test_acc = eval_model(model, test_iter)\n","print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n","\n","''' Let us now predict the sentiment on a single sentence just for the testing purpose. '''\n","test_sen1 = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n","test_sen2 = \"Ohh, such a ridiculous movie. Not gonna recommend it to anyone. Complete waste of time and money.\"\n","\n","test_sen1 = TEXT.preprocess(test_sen1)\n","test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n","\n","test_sen2 = TEXT.preprocess(test_sen2)\n","test_sen2 = [[TEXT.vocab.stoi[x] for x in test_sen2]]\n","\n","test_sen = np.asarray(test_sen1)\n","test_sen = torch.LongTensor(test_sen)\n","test_tensor = Variable(test_sen, volatile=True)\n","test_tensor = test_tensor.cuda()\n","model.eval()\n","output = model(test_tensor, 1)\n","out = F.softmax(output, 1)\n","if (torch.argmax(out[0]) == 1):\n","    print (\"Sentiment: Positive\")\n","else:\n","    print (\"Sentiment: Negative\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 1, Idx: 100, Training Loss: 0.6848, Training Accuracy:  59.38%\n","Epoch: 1, Idx: 200, Training Loss: 0.5839, Training Accuracy:  65.62%\n","Epoch: 1, Idx: 300, Training Loss: 0.7437, Training Accuracy:  50.00%\n","Epoch: 1, Idx: 400, Training Loss: 0.7261, Training Accuracy:  43.75%\n","Epoch: 1, Idx: 500, Training Loss: 0.7226, Training Accuracy:  43.75%\n","Epoch: 01, Train Loss: 0.719, Train Acc: 52.88%, Val. Loss: 0.729521, Val. Acc: 52.59%\n","Epoch: 2, Idx: 100, Training Loss: 0.6904, Training Accuracy:  65.62%\n","Epoch: 2, Idx: 200, Training Loss: 0.7528, Training Accuracy:  46.88%\n","Epoch: 2, Idx: 300, Training Loss: 0.6835, Training Accuracy:  53.12%\n","Epoch: 2, Idx: 400, Training Loss: 0.6764, Training Accuracy:  62.50%\n","Epoch: 2, Idx: 500, Training Loss: 0.7312, Training Accuracy:  43.75%\n","Epoch: 02, Train Loss: 0.719, Train Acc: 52.95%, Val. Loss: 0.693121, Val. Acc: 52.27%\n","Epoch: 3, Idx: 100, Training Loss: 0.6324, Training Accuracy:  59.38%\n","Epoch: 3, Idx: 200, Training Loss: 0.6417, Training Accuracy:  71.88%\n","Epoch: 3, Idx: 300, Training Loss: 0.7027, Training Accuracy:  53.12%\n","Epoch: 3, Idx: 400, Training Loss: 0.7308, Training Accuracy:  50.00%\n","Epoch: 3, Idx: 500, Training Loss: 0.6897, Training Accuracy:  43.75%\n","Epoch: 03, Train Loss: 0.706, Train Acc: 53.30%, Val. Loss: 0.698764, Val. Acc: 54.06%\n","Epoch: 4, Idx: 100, Training Loss: 0.7617, Training Accuracy:  46.88%\n","Epoch: 4, Idx: 200, Training Loss: 0.7403, Training Accuracy:  46.88%\n","Epoch: 4, Idx: 300, Training Loss: 0.6958, Training Accuracy:  53.12%\n","Epoch: 4, Idx: 400, Training Loss: 0.6694, Training Accuracy:  62.50%\n","Epoch: 4, Idx: 500, Training Loss: 0.7270, Training Accuracy:  46.88%\n","Epoch: 04, Train Loss: 0.711, Train Acc: 54.49%, Val. Loss: 0.753249, Val. Acc: 52.54%\n","Epoch: 5, Idx: 100, Training Loss: 0.7243, Training Accuracy:  56.25%\n","Epoch: 5, Idx: 200, Training Loss: 0.6466, Training Accuracy:  68.75%\n","Epoch: 5, Idx: 300, Training Loss: 0.7218, Training Accuracy:  56.25%\n","Epoch: 5, Idx: 400, Training Loss: 0.8258, Training Accuracy:  40.62%\n","Epoch: 5, Idx: 500, Training Loss: 0.8052, Training Accuracy:  43.75%\n","Epoch: 05, Train Loss: 0.703, Train Acc: 55.25%, Val. Loss: 0.772980, Val. Acc: 51.36%\n","Epoch: 6, Idx: 100, Training Loss: 0.8070, Training Accuracy:  43.75%\n","Epoch: 6, Idx: 200, Training Loss: 0.6722, Training Accuracy:  59.38%\n","Epoch: 6, Idx: 300, Training Loss: 0.8815, Training Accuracy:  37.50%\n","Epoch: 6, Idx: 400, Training Loss: 0.6259, Training Accuracy:  75.00%\n","Epoch: 6, Idx: 500, Training Loss: 0.8430, Training Accuracy:  50.00%\n","Epoch: 06, Train Loss: 0.678, Train Acc: 59.61%, Val. Loss: 0.683376, Val. Acc: 60.78%\n","Epoch: 7, Idx: 100, Training Loss: 0.5915, Training Accuracy:  68.75%\n","Epoch: 7, Idx: 200, Training Loss: 0.8028, Training Accuracy:  50.00%\n","Epoch: 7, Idx: 300, Training Loss: 0.6932, Training Accuracy:  56.25%\n","Epoch: 7, Idx: 400, Training Loss: 0.5543, Training Accuracy:  71.88%\n","Epoch: 7, Idx: 500, Training Loss: 0.6499, Training Accuracy:  50.00%\n","Epoch: 07, Train Loss: 0.659, Train Acc: 62.67%, Val. Loss: 0.684156, Val. Acc: 58.98%\n","Epoch: 8, Idx: 100, Training Loss: 0.5745, Training Accuracy:  71.88%\n","Epoch: 8, Idx: 200, Training Loss: 0.6635, Training Accuracy:  65.62%\n","Epoch: 8, Idx: 300, Training Loss: 0.6204, Training Accuracy:  65.62%\n","Epoch: 8, Idx: 400, Training Loss: 0.5518, Training Accuracy:  71.88%\n","Epoch: 8, Idx: 500, Training Loss: 0.7358, Training Accuracy:  46.88%\n","Epoch: 08, Train Loss: 0.661, Train Acc: 62.20%, Val. Loss: 0.689134, Val. Acc: 56.28%\n","Epoch: 9, Idx: 100, Training Loss: 0.6916, Training Accuracy:  56.25%\n","Epoch: 9, Idx: 200, Training Loss: 0.6105, Training Accuracy:  62.50%\n","Epoch: 9, Idx: 300, Training Loss: 0.6822, Training Accuracy:  56.25%\n","Epoch: 9, Idx: 400, Training Loss: 0.5888, Training Accuracy:  71.88%\n","Epoch: 9, Idx: 500, Training Loss: 0.8586, Training Accuracy:  43.75%\n","Epoch: 09, Train Loss: 0.640, Train Acc: 64.31%, Val. Loss: 0.669263, Val. Acc: 59.49%\n","Epoch: 10, Idx: 100, Training Loss: 0.6264, Training Accuracy:  68.75%\n","Epoch: 10, Idx: 200, Training Loss: 0.6298, Training Accuracy:  68.75%\n","Epoch: 10, Idx: 300, Training Loss: 0.8247, Training Accuracy:  53.12%\n","Epoch: 10, Idx: 400, Training Loss: 0.6008, Training Accuracy:  75.00%\n","Epoch: 10, Idx: 500, Training Loss: 0.6266, Training Accuracy:  68.75%\n","Epoch: 10, Train Loss: 0.660, Train Acc: 62.18%, Val. Loss: 0.738638, Val. Acc: 58.71%\n","Test Loss: 0.754, Test Acc: 58.12%\n","Sentiment: Positive\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"XjJSwuCoTUzU","colab_type":"code","outputId":"f846e48b-81da-4c99-e6b3-12db6dc9e531","executionInfo":{"status":"ok","timestamp":1587951915297,"user_tz":-540,"elapsed":121684,"user":{"displayName":"Seoyoung Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjITIRuMxnapYLrcCepnJsCF3Rb52KogXDd9B-Mhg=s64","userId":"04592586031163595245"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["learning_rate = 2e-5\n","batch_size = 32\n","output_size = 2\n","hidden_size = 256\n","embedding_length = 300\n","\n","model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n","loss_fn = F.cross_entropy\n","\n","for epoch in range(10):\n","    train_loss, train_acc = train_model(model, train_iter, epoch)\n","    val_loss, val_acc = eval_model(model, valid_iter)\n","    \n","    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n","    \n","test_loss, test_acc = eval_model(model, test_iter)\n","print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n","\n","''' Let us now predict the sentiment on a single sentence just for the testing purpose. '''\n","test_sen1 = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n","test_sen2 = \"Ohh, such a ridiculous movie. Not gonna recommend it to anyone. Complete waste of time and money.\"\n","\n","test_sen1 = TEXT.preprocess(test_sen1)\n","test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n","\n","test_sen2 = TEXT.preprocess(test_sen2)\n","test_sen2 = [[TEXT.vocab.stoi[x] for x in test_sen2]]\n","\n","test_sen = np.asarray(test_sen1)\n","test_sen = torch.LongTensor(test_sen)\n","test_tensor = Variable(test_sen, volatile=True)\n","test_tensor = test_tensor.cuda()\n","model.eval()\n","output = model(test_tensor, 1)\n","out = F.softmax(output, 1)\n","if (torch.argmax(out[0]) == 1):\n","    print (\"Sentiment: Positive\")\n","else:\n","    print (\"Sentiment: Negative\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 1, Idx: 100, Training Loss: 0.7791, Training Accuracy:  43.75%\n","Epoch: 1, Idx: 200, Training Loss: 0.6570, Training Accuracy:  65.62%\n","Epoch: 1, Idx: 300, Training Loss: 0.7062, Training Accuracy:  56.25%\n","Epoch: 1, Idx: 400, Training Loss: 0.7575, Training Accuracy:  40.62%\n","Epoch: 1, Idx: 500, Training Loss: 0.7106, Training Accuracy:  40.62%\n","Epoch: 01, Train Loss: 0.689, Train Acc: 53.21%, Val. Loss: 0.684662, Val. Acc: 53.11%\n","Epoch: 2, Idx: 100, Training Loss: 0.6196, Training Accuracy:  65.62%\n","Epoch: 2, Idx: 200, Training Loss: 0.7016, Training Accuracy:  46.88%\n","Epoch: 2, Idx: 300, Training Loss: 0.6711, Training Accuracy:  68.75%\n","Epoch: 2, Idx: 400, Training Loss: 0.6667, Training Accuracy:  62.50%\n","Epoch: 2, Idx: 500, Training Loss: 0.6810, Training Accuracy:  46.88%\n","Epoch: 02, Train Loss: 0.673, Train Acc: 56.39%, Val. Loss: 0.686161, Val. Acc: 53.83%\n","Epoch: 3, Idx: 100, Training Loss: 0.6876, Training Accuracy:  68.75%\n","Epoch: 3, Idx: 200, Training Loss: 0.4701, Training Accuracy:  78.12%\n","Epoch: 3, Idx: 300, Training Loss: 0.5311, Training Accuracy:  78.12%\n","Epoch: 3, Idx: 400, Training Loss: 0.3663, Training Accuracy:  90.62%\n","Epoch: 3, Idx: 500, Training Loss: 0.4437, Training Accuracy:  81.25%\n","Epoch: 03, Train Loss: 0.513, Train Acc: 75.35%, Val. Loss: 0.433439, Val. Acc: 80.60%\n","Epoch: 4, Idx: 100, Training Loss: 0.3994, Training Accuracy:  81.25%\n","Epoch: 4, Idx: 200, Training Loss: 0.1714, Training Accuracy:  96.88%\n","Epoch: 4, Idx: 300, Training Loss: 0.4470, Training Accuracy:  78.12%\n","Epoch: 4, Idx: 400, Training Loss: 0.1892, Training Accuracy:  100.00%\n","Epoch: 4, Idx: 500, Training Loss: 0.2588, Training Accuracy:  87.50%\n","Epoch: 04, Train Loss: 0.375, Train Acc: 83.68%, Val. Loss: 0.369348, Val. Acc: 83.02%\n","Epoch: 5, Idx: 100, Training Loss: 0.2903, Training Accuracy:  90.62%\n","Epoch: 5, Idx: 200, Training Loss: 0.4105, Training Accuracy:  81.25%\n","Epoch: 5, Idx: 300, Training Loss: 0.4219, Training Accuracy:  81.25%\n","Epoch: 5, Idx: 400, Training Loss: 0.2723, Training Accuracy:  84.38%\n","Epoch: 5, Idx: 500, Training Loss: 0.5178, Training Accuracy:  75.00%\n","Epoch: 05, Train Loss: 0.333, Train Acc: 85.71%, Val. Loss: 0.355831, Val. Acc: 83.74%\n","Epoch: 6, Idx: 100, Training Loss: 0.2921, Training Accuracy:  90.62%\n","Epoch: 6, Idx: 200, Training Loss: 0.4514, Training Accuracy:  81.25%\n","Epoch: 6, Idx: 300, Training Loss: 0.0915, Training Accuracy:  93.75%\n","Epoch: 6, Idx: 400, Training Loss: 0.2025, Training Accuracy:  96.88%\n","Epoch: 6, Idx: 500, Training Loss: 0.2202, Training Accuracy:  90.62%\n","Epoch: 06, Train Loss: 0.296, Train Acc: 87.60%, Val. Loss: 0.366597, Val. Acc: 84.27%\n","Epoch: 7, Idx: 100, Training Loss: 0.2344, Training Accuracy:  93.75%\n","Epoch: 7, Idx: 200, Training Loss: 0.1376, Training Accuracy:  96.88%\n","Epoch: 7, Idx: 300, Training Loss: 0.2347, Training Accuracy:  90.62%\n","Epoch: 7, Idx: 400, Training Loss: 0.4961, Training Accuracy:  71.88%\n","Epoch: 7, Idx: 500, Training Loss: 0.3480, Training Accuracy:  78.12%\n","Epoch: 07, Train Loss: 0.257, Train Acc: 89.48%, Val. Loss: 0.368623, Val. Acc: 83.62%\n","Epoch: 8, Idx: 100, Training Loss: 0.1956, Training Accuracy:  93.75%\n","Epoch: 8, Idx: 200, Training Loss: 0.1874, Training Accuracy:  96.88%\n","Epoch: 8, Idx: 300, Training Loss: 0.2192, Training Accuracy:  96.88%\n","Epoch: 8, Idx: 400, Training Loss: 0.2677, Training Accuracy:  90.62%\n","Epoch: 8, Idx: 500, Training Loss: 0.1852, Training Accuracy:  93.75%\n","Epoch: 08, Train Loss: 0.206, Train Acc: 92.01%, Val. Loss: 0.427978, Val. Acc: 82.19%\n","Epoch: 9, Idx: 100, Training Loss: 0.1599, Training Accuracy:  93.75%\n","Epoch: 9, Idx: 200, Training Loss: 0.1545, Training Accuracy:  90.62%\n","Epoch: 9, Idx: 300, Training Loss: 0.2731, Training Accuracy:  87.50%\n","Epoch: 9, Idx: 400, Training Loss: 0.3064, Training Accuracy:  93.75%\n","Epoch: 9, Idx: 500, Training Loss: 0.2046, Training Accuracy:  93.75%\n","Epoch: 09, Train Loss: 0.157, Train Acc: 94.12%, Val. Loss: 0.428974, Val. Acc: 83.06%\n","Epoch: 10, Idx: 100, Training Loss: 0.0873, Training Accuracy:  93.75%\n","Epoch: 10, Idx: 200, Training Loss: 0.0353, Training Accuracy:  100.00%\n","Epoch: 10, Idx: 300, Training Loss: 0.0306, Training Accuracy:  100.00%\n","Epoch: 10, Idx: 400, Training Loss: 0.1497, Training Accuracy:  93.75%\n","Epoch: 10, Idx: 500, Training Loss: 0.0361, Training Accuracy:  100.00%\n","Epoch: 10, Train Loss: 0.117, Train Acc: 95.74%, Val. Loss: 0.576948, Val. Acc: 82.51%\n","Test Loss: 0.545, Test Acc: 84.06%\n","Sentiment: Positive\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-Er906HdaCew","colab_type":"code","outputId":"2f1fb9aa-a14c-4931-fecf-df77401e3e22","executionInfo":{"status":"ok","timestamp":1587956220032,"user_tz":-540,"elapsed":121653,"user":{"displayName":"Seoyoung Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjITIRuMxnapYLrcCepnJsCF3Rb52KogXDd9B-Mhg=s64","userId":"04592586031163595245"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["learning_rate = 0.01\n","batch_size = 32\n","output_size = 2\n","hidden_size = 256\n","embedding_length = 300\n","\n","model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n","loss_fn = F.cross_entropy\n","\n","for epoch in range(10):\n","    train_loss, train_acc = train_model(model, train_iter, epoch)\n","    val_loss, val_acc = eval_model(model, valid_iter)\n","    \n","    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n","    \n","test_loss, test_acc = eval_model(model, test_iter)\n","print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n","\n","''' Let us now predict the sentiment on a single sentence just for the testing purpose. '''\n","test_sen1 = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n","test_sen2 = \"Ohh, such a ridiculous movie. Not gonna recommend it to anyone. Complete waste of time and money.\"\n","\n","test_sen1 = TEXT.preprocess(test_sen1)\n","test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n","\n","test_sen2 = TEXT.preprocess(test_sen2)\n","test_sen2 = [[TEXT.vocab.stoi[x] for x in test_sen2]]\n","\n","test_sen = np.asarray(test_sen1)\n","test_sen = torch.LongTensor(test_sen)\n","test_tensor = Variable(test_sen, volatile=True)\n","test_tensor = test_tensor.cuda()\n","model.eval()\n","output = model(test_tensor, 1)\n","out = F.softmax(output, 1)\n","if (torch.argmax(out[0]) == 1):\n","    print (\"Sentiment: Positive\")\n","else:\n","    print (\"Sentiment: Negative\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 1, Idx: 100, Training Loss: 0.6714, Training Accuracy:  62.50%\n","Epoch: 1, Idx: 200, Training Loss: 0.7192, Training Accuracy:  40.62%\n","Epoch: 1, Idx: 300, Training Loss: 0.6867, Training Accuracy:  53.12%\n","Epoch: 1, Idx: 400, Training Loss: 0.7455, Training Accuracy:  43.75%\n","Epoch: 1, Idx: 500, Training Loss: 0.6863, Training Accuracy:  59.38%\n","Epoch: 01, Train Loss: 0.677, Train Acc: 56.06%, Val. Loss: 0.591983, Val. Acc: 71.69%\n","Epoch: 2, Idx: 100, Training Loss: 0.4222, Training Accuracy:  87.50%\n","Epoch: 2, Idx: 200, Training Loss: 0.5470, Training Accuracy:  81.25%\n","Epoch: 2, Idx: 300, Training Loss: 0.5423, Training Accuracy:  78.12%\n","Epoch: 2, Idx: 400, Training Loss: 0.6992, Training Accuracy:  46.88%\n","Epoch: 2, Idx: 500, Training Loss: 0.6310, Training Accuracy:  68.75%\n","Epoch: 02, Train Loss: 0.613, Train Acc: 66.77%, Val. Loss: 0.561424, Val. Acc: 74.60%\n","Epoch: 3, Idx: 100, Training Loss: 0.3317, Training Accuracy:  93.75%\n","Epoch: 3, Idx: 200, Training Loss: 0.4093, Training Accuracy:  81.25%\n","Epoch: 3, Idx: 300, Training Loss: 0.4990, Training Accuracy:  75.00%\n","Epoch: 3, Idx: 400, Training Loss: 0.2826, Training Accuracy:  84.38%\n","Epoch: 3, Idx: 500, Training Loss: 0.3555, Training Accuracy:  84.38%\n","Epoch: 03, Train Loss: 0.427, Train Acc: 81.04%, Val. Loss: 0.378021, Val. Acc: 82.87%\n","Epoch: 4, Idx: 100, Training Loss: 0.2666, Training Accuracy:  93.75%\n","Epoch: 4, Idx: 200, Training Loss: 0.3667, Training Accuracy:  78.12%\n","Epoch: 4, Idx: 300, Training Loss: 0.2414, Training Accuracy:  90.62%\n","Epoch: 4, Idx: 400, Training Loss: 0.1546, Training Accuracy:  96.88%\n","Epoch: 4, Idx: 500, Training Loss: 0.3003, Training Accuracy:  84.38%\n","Epoch: 04, Train Loss: 0.349, Train Acc: 84.73%, Val. Loss: 0.357616, Val. Acc: 83.71%\n","Epoch: 5, Idx: 100, Training Loss: 0.2983, Training Accuracy:  90.62%\n","Epoch: 5, Idx: 200, Training Loss: 0.6431, Training Accuracy:  78.12%\n","Epoch: 5, Idx: 300, Training Loss: 0.2188, Training Accuracy:  90.62%\n","Epoch: 5, Idx: 400, Training Loss: 0.3302, Training Accuracy:  87.50%\n","Epoch: 5, Idx: 500, Training Loss: 0.4078, Training Accuracy:  81.25%\n","Epoch: 05, Train Loss: 0.315, Train Acc: 86.66%, Val. Loss: 0.378922, Val. Acc: 82.93%\n","Epoch: 6, Idx: 100, Training Loss: 0.2382, Training Accuracy:  87.50%\n","Epoch: 6, Idx: 200, Training Loss: 0.3258, Training Accuracy:  87.50%\n","Epoch: 6, Idx: 300, Training Loss: 0.2238, Training Accuracy:  87.50%\n","Epoch: 6, Idx: 400, Training Loss: 0.2684, Training Accuracy:  90.62%\n","Epoch: 6, Idx: 500, Training Loss: 0.2018, Training Accuracy:  93.75%\n","Epoch: 06, Train Loss: 0.279, Train Acc: 88.56%, Val. Loss: 0.364408, Val. Acc: 84.30%\n","Epoch: 7, Idx: 100, Training Loss: 0.2612, Training Accuracy:  93.75%\n","Epoch: 7, Idx: 200, Training Loss: 0.1440, Training Accuracy:  96.88%\n","Epoch: 7, Idx: 300, Training Loss: 0.1331, Training Accuracy:  93.75%\n","Epoch: 7, Idx: 400, Training Loss: 0.2497, Training Accuracy:  87.50%\n","Epoch: 7, Idx: 500, Training Loss: 0.2228, Training Accuracy:  93.75%\n","Epoch: 07, Train Loss: 0.242, Train Acc: 90.09%, Val. Loss: 0.369518, Val. Acc: 84.07%\n","Epoch: 8, Idx: 100, Training Loss: 0.2186, Training Accuracy:  90.62%\n","Epoch: 8, Idx: 200, Training Loss: 0.0741, Training Accuracy:  96.88%\n","Epoch: 8, Idx: 300, Training Loss: 0.1178, Training Accuracy:  96.88%\n","Epoch: 8, Idx: 400, Training Loss: 0.2609, Training Accuracy:  93.75%\n","Epoch: 8, Idx: 500, Training Loss: 0.2550, Training Accuracy:  90.62%\n","Epoch: 08, Train Loss: 0.199, Train Acc: 92.28%, Val. Loss: 0.395112, Val. Acc: 83.79%\n","Epoch: 9, Idx: 100, Training Loss: 0.2593, Training Accuracy:  93.75%\n","Epoch: 9, Idx: 200, Training Loss: 0.1544, Training Accuracy:  93.75%\n","Epoch: 9, Idx: 300, Training Loss: 0.2241, Training Accuracy:  93.75%\n","Epoch: 9, Idx: 400, Training Loss: 0.1695, Training Accuracy:  93.75%\n","Epoch: 9, Idx: 500, Training Loss: 0.1023, Training Accuracy:  96.88%\n","Epoch: 09, Train Loss: 0.157, Train Acc: 94.11%, Val. Loss: 0.487608, Val. Acc: 83.71%\n","Epoch: 10, Idx: 100, Training Loss: 0.0795, Training Accuracy:  96.88%\n","Epoch: 10, Idx: 200, Training Loss: 0.0958, Training Accuracy:  96.88%\n","Epoch: 10, Idx: 300, Training Loss: 0.2357, Training Accuracy:  87.50%\n","Epoch: 10, Idx: 400, Training Loss: 0.2465, Training Accuracy:  87.50%\n","Epoch: 10, Idx: 500, Training Loss: 0.1738, Training Accuracy:  96.88%\n","Epoch: 10, Train Loss: 0.116, Train Acc: 95.77%, Val. Loss: 0.450358, Val. Acc: 83.44%\n","Test Loss: 0.450, Test Acc: 83.86%\n","Sentiment: Positive\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"44fNkF5iXxzE","colab_type":"code","outputId":"594a5615-9056-48e4-919f-f1d718e34464","executionInfo":{"status":"ok","timestamp":1587956563183,"user_tz":-540,"elapsed":76218,"user":{"displayName":"Seoyoung Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjITIRuMxnapYLrcCepnJsCF3Rb52KogXDd9B-Mhg=s64","userId":"04592586031163595245"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["learning_rate = 2e-5\n","batch_size = 32\n","output_size = 2\n","in_channels = 1\n","out_channels = 100\n","kernel_heights = [3, 4, 5]\n","stride = 1\n","padding = 0\n","keep_probab = 0.5\n","embedding_length = 300\n","\n","model = CNN(batch_size, output_size, in_channels, out_channels, kernel_heights, stride, padding, keep_probab, vocab_size, embedding_length, word_embeddings)\n","loss_fn = F.cross_entropy\n","\n","for epoch in range(10):\n","    train_loss, train_acc = train_model(model, train_iter, epoch)\n","    val_loss, val_acc = eval_model(model, valid_iter)\n","    \n","    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n","    \n","test_loss, test_acc = eval_model(model, test_iter)\n","print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n","\n","''' Let us now predict the sentiment on a single sentence just for the testing purpose. '''\n","test_sen1 = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n","test_sen2 = \"Ohh, such a ridiculous movie. Not gonna recommend it to anyone. Complete waste of time and money.\"\n","\n","test_sen1 = TEXT.preprocess(test_sen1)\n","test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n","\n","test_sen2 = TEXT.preprocess(test_sen2)\n","test_sen2 = [[TEXT.vocab.stoi[x] for x in test_sen2]]\n","\n","test_sen = np.asarray(test_sen1)\n","test_sen = torch.LongTensor(test_sen)\n","test_tensor = Variable(test_sen, volatile=True)\n","test_tensor = test_tensor.cuda()\n","model.eval()\n","output = model(test_tensor, 1)\n","out = F.softmax(output, 1)\n","if (torch.argmax(out[0]) == 1):\n","    print (\"Sentiment: Positive\")\n","else:\n","    print (\"Sentiment: Negative\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 1, Idx: 100, Training Loss: 0.4990, Training Accuracy:  78.12%\n","Epoch: 1, Idx: 200, Training Loss: 0.3390, Training Accuracy:  81.25%\n","Epoch: 1, Idx: 300, Training Loss: 0.4174, Training Accuracy:  84.38%\n","Epoch: 1, Idx: 400, Training Loss: 0.8598, Training Accuracy:  71.88%\n","Epoch: 1, Idx: 500, Training Loss: 0.4299, Training Accuracy:  68.75%\n","Epoch: 01, Train Loss: 0.480, Train Acc: 76.12%, Val. Loss: 0.367502, Val. Acc: 83.09%\n","Epoch: 2, Idx: 100, Training Loss: 0.1965, Training Accuracy:  93.75%\n","Epoch: 2, Idx: 200, Training Loss: 0.3834, Training Accuracy:  84.38%\n","Epoch: 2, Idx: 300, Training Loss: 0.5098, Training Accuracy:  78.12%\n","Epoch: 2, Idx: 400, Training Loss: 0.3701, Training Accuracy:  81.25%\n","Epoch: 2, Idx: 500, Training Loss: 0.6513, Training Accuracy:  75.00%\n","Epoch: 02, Train Loss: 0.369, Train Acc: 83.49%, Val. Loss: 0.358045, Val. Acc: 83.87%\n","Epoch: 3, Idx: 100, Training Loss: 0.3787, Training Accuracy:  78.12%\n","Epoch: 3, Idx: 200, Training Loss: 0.3007, Training Accuracy:  93.75%\n","Epoch: 3, Idx: 300, Training Loss: 0.2868, Training Accuracy:  90.62%\n","Epoch: 3, Idx: 400, Training Loss: 0.2438, Training Accuracy:  90.62%\n","Epoch: 3, Idx: 500, Training Loss: 0.2718, Training Accuracy:  90.62%\n","Epoch: 03, Train Loss: 0.310, Train Acc: 86.69%, Val. Loss: 0.362774, Val. Acc: 83.79%\n","Epoch: 4, Idx: 100, Training Loss: 0.1868, Training Accuracy:  93.75%\n","Epoch: 4, Idx: 200, Training Loss: 0.2089, Training Accuracy:  90.62%\n","Epoch: 4, Idx: 300, Training Loss: 0.3050, Training Accuracy:  87.50%\n","Epoch: 4, Idx: 400, Training Loss: 0.1601, Training Accuracy:  96.88%\n","Epoch: 4, Idx: 500, Training Loss: 0.1849, Training Accuracy:  90.62%\n","Epoch: 04, Train Loss: 0.263, Train Acc: 89.03%, Val. Loss: 0.378749, Val. Acc: 83.60%\n","Epoch: 5, Idx: 100, Training Loss: 0.3051, Training Accuracy:  84.38%\n","Epoch: 5, Idx: 200, Training Loss: 0.2493, Training Accuracy:  90.62%\n","Epoch: 5, Idx: 300, Training Loss: 0.3966, Training Accuracy:  87.50%\n","Epoch: 5, Idx: 400, Training Loss: 0.2423, Training Accuracy:  93.75%\n","Epoch: 5, Idx: 500, Training Loss: 0.2061, Training Accuracy:  87.50%\n","Epoch: 05, Train Loss: 0.214, Train Acc: 90.94%, Val. Loss: 0.424452, Val. Acc: 82.81%\n","Epoch: 6, Idx: 100, Training Loss: 0.2182, Training Accuracy:  90.62%\n","Epoch: 6, Idx: 200, Training Loss: 0.0831, Training Accuracy:  96.88%\n","Epoch: 6, Idx: 300, Training Loss: 0.1031, Training Accuracy:  96.88%\n","Epoch: 6, Idx: 400, Training Loss: 0.1526, Training Accuracy:  90.62%\n","Epoch: 6, Idx: 500, Training Loss: 0.0665, Training Accuracy:  100.00%\n","Epoch: 06, Train Loss: 0.178, Train Acc: 92.66%, Val. Loss: 0.390423, Val. Acc: 84.64%\n","Epoch: 7, Idx: 100, Training Loss: 0.1061, Training Accuracy:  93.75%\n","Epoch: 7, Idx: 200, Training Loss: 0.1578, Training Accuracy:  93.75%\n","Epoch: 7, Idx: 300, Training Loss: 0.1350, Training Accuracy:  93.75%\n","Epoch: 7, Idx: 400, Training Loss: 0.2318, Training Accuracy:  90.62%\n","Epoch: 7, Idx: 500, Training Loss: 0.1579, Training Accuracy:  93.75%\n","Epoch: 07, Train Loss: 0.156, Train Acc: 93.73%, Val. Loss: 0.402335, Val. Acc: 85.20%\n","Epoch: 8, Idx: 100, Training Loss: 0.0370, Training Accuracy:  100.00%\n","Epoch: 8, Idx: 200, Training Loss: 0.2317, Training Accuracy:  90.62%\n","Epoch: 8, Idx: 300, Training Loss: 0.0331, Training Accuracy:  100.00%\n","Epoch: 8, Idx: 400, Training Loss: 0.0616, Training Accuracy:  96.88%\n","Epoch: 8, Idx: 500, Training Loss: 0.1185, Training Accuracy:  90.62%\n","Epoch: 08, Train Loss: 0.140, Train Acc: 94.35%, Val. Loss: 0.444577, Val. Acc: 84.68%\n","Epoch: 9, Idx: 100, Training Loss: 0.0268, Training Accuracy:  100.00%\n","Epoch: 9, Idx: 200, Training Loss: 0.2566, Training Accuracy:  90.62%\n","Epoch: 9, Idx: 300, Training Loss: 0.1026, Training Accuracy:  96.88%\n","Epoch: 9, Idx: 400, Training Loss: 0.3014, Training Accuracy:  78.12%\n","Epoch: 9, Idx: 500, Training Loss: 0.2548, Training Accuracy:  84.38%\n","Epoch: 09, Train Loss: 0.129, Train Acc: 95.06%, Val. Loss: 0.440448, Val. Acc: 85.04%\n","Epoch: 10, Idx: 100, Training Loss: 0.1142, Training Accuracy:  100.00%\n","Epoch: 10, Idx: 200, Training Loss: 0.1197, Training Accuracy:  93.75%\n","Epoch: 10, Idx: 300, Training Loss: 0.2110, Training Accuracy:  90.62%\n","Epoch: 10, Idx: 400, Training Loss: 0.2757, Training Accuracy:  81.25%\n","Epoch: 10, Idx: 500, Training Loss: 0.0318, Training Accuracy:  100.00%\n","Epoch: 10, Train Loss: 0.112, Train Acc: 95.57%, Val. Loss: 0.471756, Val. Acc: 84.26%\n","Test Loss: 0.471, Test Acc: 84.07%\n","Sentiment: Positive\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"],"name":"stderr"}]}]}