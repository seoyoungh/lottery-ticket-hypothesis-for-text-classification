# Lottery Ticket Hypothesis for Text Classification

This study is being conducted in ``Software Convergence Capstone Design - Spring 2020`` class.

## Researcher
* [Seoyoung Hong](https://github.com/seoyoungh) from Kyunghee Univ.

## Overview
ICLR 2019 best paperë¡œ ì„ ì •ëœ â€‹``The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks`` ì—°êµ¬ëŠ” network pruning ë¶„ì•¼ì—ì„œ ì£¼ëª©ì„ ë°›ì•˜ë‹¤. ì²˜ìŒ weight initializationì—ì„œ ì‚¬ìš©ëœ ì´ˆê¸° weightë¥¼ ì €ì¥í•´, ì´í›„ subnetworkì˜ weightë¡œ ë‹¤ì‹œ ë„£ì–´ì£¼ëŠ” ê°„ë‹¨í•œ ë°©ë²•ì´ì§€ë§Œ ê·¸ ì„±ëŠ¥ì€ ë›°ì–´ë‚¬ë‹¤. í•˜ì§€ë§Œ, í•´ë‹¹ ì—°êµ¬ëŠ” ``image classification`` taskì— ``CNN``ì„ ì ìš©í•˜ëŠ” ê²½ìš°ì— ëŒ€í•´ì„œë§Œ ì„±ëŠ¥ì„ ê²€ì¦í–ˆë‹¤.

ë‹¤ë¥¸ ì—°êµ¬ì¸ ``PLAYING THE LOTTERY WITH REWARDS AND MULTIPLE LANGUAGES: LOTTERY TICKETS IN RL AND NLP``ì—ì„œ NLP taskì— í•´ë‹¹ ê°€ì„¤ì„ ì ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ê°€ëŠ¥ì„±ì„ ë³´ì˜€ë‹¤. ì´ ì—°êµ¬ì—ì„œëŠ” `` language modeling`` taskì— ``LSTM``ì„, ``machine translation`` taskì—ëŠ” ``transformer``ë¥¼ ì‚¬ìš©í•´ ë‘ ëª¨ë¸ì—ì„œ lottery ticket hypothesisë¥¼ ê²€ì¦í–ˆë‹¤.

í•˜ì§€ë§Œ ì•„ì§ NLP ë¶„ì•¼ì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì‹ ê²½ë§ ëª¨ë¸, ë‹¤ì–‘í•œ taskì— í•´ë‹¹ ê°€ì„¤ì„ ë²”ìš©ì ìœ¼ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ”ì§€ëŠ” ì•„ì§ ë¶ˆë¶„ëª…í•˜ë‹¤. ë”°ë¼ì„œ, lottery ticket ê¸°ë²•ì˜ NLP ë¶„ì•¼ì— ëŒ€í•œ ë²”ìš©ì„±ì„ í™•ì¸í•˜ê¸° ìœ„í•´ ì—°êµ¬ë¥¼ ì§„í–‰í•˜ê²Œ ë˜ì—ˆë‹¤.

### Needs
#### Model Compression in NLP
ëª¨ë¸ ê²½ëŸ‰í™” ê¸°ë²•ì€ CNN ëª¨ë¸ì„ ì£¼ë¡œ ì“°ëŠ” computer visionê³¼ ê°™ì€ ë¶„ì•¼ì—ì„œ ì—°êµ¬ê°€ ì§„í–‰ë˜ì–´ ì™”ë‹¤. RNN ê³„ì—´ì˜ ëª¨ë¸ì„ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” NLP ë¶„ì•¼ì—ì„œëŠ” ê·¸ í•„ìš”ì„±ì´ ë§¤ìš° ê°•ì¡°ë˜ì§€ëŠ” ì•Šì•˜ì—ˆë‹¤. RNNì€ ë‹¤ìŒ ì…ë ¥ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•´ ì´ì „ ë°ì´í„°ê°€ í•„ìš”í•˜ì—¬ ë³‘ë ¬í™”ê°€ ì–´ë µë‹¤ëŠ” í•œê³„ì  ë•Œë¬¸ì— ëª¨ë¸ì„ ê±°ëŒ€í™”í•˜ê¸° ì–´ë ¤ì› ê¸° ë•Œë¬¸ì´ë‹¤. í•˜ì§€ë§Œ, Googleì´ ``BERT``ë¥¼ ì¶œì‹œí•œ í›„ NLP ë¶„ì•¼ì—ì„œë„ ê±°ëŒ€í•œ ëª¨ë¸ë“¤ì´ ë“±ì¥í•˜ê¸° ì‹œì‘í–ˆë‹¤. ëª¨ë¸ì˜ ì‚¬ì „ í•™ìŠµ&ì¬í•™ìŠµì´ ê°€ëŠ¥í•´ì¡Œê³ , ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤. ë˜í•œ í° ë°°ì¹˜ ì‚¬ì´ì¦ˆê°€ í•™ìŠµì— íš¨ê³¼ì ì´ë¼ëŠ” ì˜ê²¬ì´ ë‚˜ì˜¤ë©´ì„œ ì‚¬ì „ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” ë°°ì¹˜ í¬ê¸°ê°€ ì ì  ì»¤ì§€ëŠ” ì¶”ì„¸ë¥¼ ë³´ì´ê³  ìˆë‹¤.

ë©”ëª¨ë¦¬ ë¶€ë‹´ì„ ì¤„ì´ê³ , í•™ìŠµ ì†Œìš” ì‹œê°„ì„ ì¤„ì´ê³ , ëª¨ë°”ì¼/embedded í™˜ê²½ì— ë°°ì¹˜í•˜ê¸° ìœ„í•´ì„œëŠ” parametersë¥¼ ì¤„ì´ëŠ” ëª¨ë¸ ê²½ëŸ‰í™”ê°€ í•„ìˆ˜ì ì´ë‹¤. NLP ë¶„ì•¼ì—ì„œë„ ëª¨ë¸ ê²½ëŸ‰í™” ì—°êµ¬ê°€ ë”ìš± í™œë°œíˆ ì§„í–‰ë˜ì–´ì•¼ í•œë‹¤.


### Goals
ì„ ì • ëª¨ë¸ë¡œ  ``text classification``ì„ ì§„í–‰í•  ë•Œ, pruning ê³¼ì •ì—ì„œ ``random initialization``ë³´ë‹¤ ``lottery ticket initialization`` ë°©ë²•ì´ ì„±ëŠ¥ì´ ë†’ìŒì„ ê²€ì¦í•œë‹¤.

ì¶”ê°€ë¡œ, íŠ¹ì • ì‹ ê²½ë§ ëª¨ë¸ì—ì„œ ``lottery ticket initialization``ì˜ ì„±ëŠ¥ì„ ë” ë†’ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ë°œê²¬í•˜ê²Œ ë˜ë©´ ì´ë¥¼ ì œì‹œí•œë‹¤.

## Methods
1) ê° modelì— datasetì„ í•™ìŠµì‹œí‚¨ í›„, ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.  
2) ``random initialization``ìœ¼ë¡œ pruningì„ ì§„í–‰í•œ í›„ ì„±ëŠ¥ì„ ë¹„êµí•œë‹¤.  
3) ``winning ticket initialization``ìœ¼ë¡œ pruningì„ ì§„í–‰í•œ í›„ ì„±ëŠ¥ì„ ë¹„êµí•œë‹¤.

### Models
``CNN``, ``RNN``, ``LSTM``, ``BERT``

### Task
#### : Text classification (Sentiment Analysis)

##### task 1) Binary-class text classification
* dataset: IMDb Large Movie Review Dataset, ~~Yelp Review Polarity~~

##### task 2) Multi-class text classification
* dataset: AG News, ~~DBPedia~~

### Datasets
| Dataset | Classes | Train samples | Test samples |
|---------|---------|---------------|--------------|
| AG News | 4 | 120,000 | 7,600 |
| IMDb Large Movie Review Dataset | 2 | 250,000 | 250,000 |
| Yelp Review Polarity | 2 | 560,000 | 38,000 |
| ~~DBPedia~~ | 14 | 560,000 | 70,000 |


[IMDb Dataset source](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)  
[Other Datasets source](https://course.fast.ai/datasets)

## Schedule

### Timeline
* **March**
  - ì—°êµ¬ ì£¼ì œ research â­•ï¸
  - References ì°¾ê¸° â­•ï¸

* **April**
  - ì—°êµ¬ task, model, dataset ì„ ì • â­•ï¸
  - training - ``CNN``, ``RNN``, ``LSTM``
    - task1 ``IMDb`` â­•ï¸
    - task2 ``Yelp-5`` ğŸ”œ
  - random pruning - ``CNN``, ``RNN``, ``LSTM``
    - task1 ``IMDb`` ğŸ”œ
    - task2 ``Yelp-5`` ğŸ”œ

* **May**
  - lottery ticket pruning - ``CNN``, ``RNN``, ``LSTM``
    - task1 ``IMDb``
    - task2``Yelp-5``
  - working on ``BERT``
    - training
      - task1
      - task2
    - random pruning
      - task1
      - task2
    - lottery ticket
      - task1
      - task2
  - ê° caseë³„ ì„±ëŠ¥ ë¹„êµ
  - Performace ê°œì„ 
    - ì²˜ìŒ trainingí•  ë•Œ ì¤„ ìˆ˜ ìˆëŠ” better condition ê³ ë ¤
    - hyperparameters ë°”ê¾¸ì–´ë³´ê¸°
    - ë” íš¨ê³¼ì ì¸ pruning ë°©ë²• research (especailly LSTM, BERT)
  - Consider to work with the other two datasets

* **June**
  - ``Conclusion``
      - ìµœì¢… setting ì±„íƒ
      - ìµœì¢… ì„±ëŠ¥ ë„ì¶œ
  - ``Code Encapsulation/Generalisation``
  - ``Github Deployment``
  - ê²°ê³¼ ë³´ê³ ì„œ ì‘ì„±

### Progress Report

| April |  May  | June  |
|-------|-------|-------|
| | [Week8](/assets/progress/week8.md) | [Week12](/assets/progress/week12.md) |
| | [Week9](/assets/progress/week9.md) | [Week13](/assets/progress/week13.md) |
| [Week6](/assets/progress/week6.md) | [Week10](/assets/progress/week10.md) | [Week14](/assets/progress/week14.md) |
| [Week7 (ì¤‘ê°„ ë³´ê³ )](/assets/progress/week7.md) | [Week11](/assets/progress/week11.md) | [Week15](/assets/progress/week15.md) |

## References
### Lottery Ticket Hypothesis
* [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635) ``The Main Paper``
* [PLAYING THE LOTTERY WITH REWARDS AND MULTIPLE LANGUAGES: LOTTERY TICKETS IN RL AND NLP](https://arxiv.org/abs/1906.02768)
* [Evaluating Lottery Tickets Under Distributional Shifts](https://arxiv.org/abs/1910.12708)
* [Finding Winning Tickets with Limited (or No) Supervision](https://openreview.net/forum?id=SJx_QJHYDB)

### Pruning
* [Rethinking the Value of Network Pruning](https://arxiv.org/abs/1810.05270)
* [Exploring Sparsity in Recurrent Neural Networks](https://arxiv.org/abs/1704.05119)
* [COMPARING REWINDING AND FINE-TUNING IN NEURAL NETWORK PRUNING](https://arxiv.org/abs/2003.02389)
* [C-LSTM: Enabling Efficient LSTM using Structured Compression Techniques on FPGAs](https://arxiv.org/abs/1803.06305)
* [COMPRESSING BERT: STUDYING THE EFFECTS OF WEIGHT PRUNING ON TRANSFER LEARNING](https://arxiv.org/abs/2002.08307)

### Sentiment Analysis
* [Multiclass Sentiment Prediction using Yelp Business Reviews](https://www.semanticscholar.org/paper/Multiclass-Sentiment-Prediction-using-Yelp-Business-Yu/dfa617c7c7e3a53d90c092cef09b2ee1614317a2)

### Posts
* [PyTorch Offical Libary - torchtext](https://pytorch.org/text/index.html)
* [Multi-label Text Classification using BERT](https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d)
* [Multiclass Text Classification using LSTM in Pytorch](https://towardsdatascience.com/multiclass-text-classification-using-lstm-in-pytorch-eac56baed8df)
* [Compressing and regularizing deep neural networks](https://www.oreilly.com/content/compressing-and-regularizing-deep-neural-networks/)
* [ë”¥ëŸ¬ë‹ ëª¨ë¸ ì••ì¶• ë°©ë²•ë¡ ê³¼ BERT ì••ì¶•](https://blog.est.ai/2020/03/ë”¥ëŸ¬ë‹-ëª¨ë¸-ì••ì¶•-ë°©ë²•ë¡ ê³¼-bert-ì••ì¶•/)
* [torch.nn.utils.prune ëª¨ë“ˆë¡œ BERT ë‹¤ì´ì–´íŠ¸ ì‹œí‚¤ê¸°](https://huffon.github.io/2020/03/15/torch-pruning/)

## License
